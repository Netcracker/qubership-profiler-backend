# CDT Data generator

- CLI tool to generate calls in specified time range as Parquet files and upload them to S3-compatible storage

## Usage

### Gathering data

See `doc/gathering_data.md` for information about catching actual services communication with `tcpdump`

See `doc/preparing_data.md` for information about extracting load generator data from tcp dumps

## CLI tool

This tool generates test data based on real data and uploads it to an S3 storage. The generation is done according to
the design, and the structure of the buckets is also described in the design.

### Prerequisite

1. Go 1.21.1 or later
2. A command terminal
3. Create custom database in Postgres and import tables
   from [schema](https://github.com/Netcracker/qubership-profiler-backend/apps/data-generator/-/blob/master/.docker/postgres/opt/cdt_schema.sql).
4. Create bucket `profiler` in S3-compatible storage

### How to build and run application

1. Clone git repository
2. Open directory in terminal
3. Execute these commands:

   ```bash
   go build -o ./bin/data-generator -ldflags="-w -s"

   ./bin/data-generator run --startdate <start date in the past> --minio.url=<endpoint> --minio.key=<access_key> --minio.secret=<secret_access_key>
   ```

### Possible flags

| Command            | Default                                                                   | Description                                                                                                          |
|--------------------|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|
| `--help`           |                                                                           | How to use tool                                                                                                      |
| `--clear`          | `true`                                                                    | **Important** Will clear old data from previous runs in `output` directory                                           |
| `--parse`          | `false`                                                                   | Only parse original binary data from tcp-dump                                                                        |
| `--startdate`      |                                                                           | **Required** Generate data starting from this day and upload them according to the appropriate buckets to s3 storage |
| `--enddate`        | current time                                                              | End date. If empty, it will be set to the current time and hourly and 5-minute buckets will be created on S3.        |
| `--starttime`      |                                                                           | **Optional** If set, generate recent data starting from this hour and upload them to Postgres **(only!)**            |
| `--hours`          | 1                                                                         | **Optional** If set, generate recent data for that amount of hours                                                   |
| `--ns`             | `test.namespace`                                                          | The prefix for the namespace name                                                                                    |
| `--svc`            | `test.service`                                                            | The prefix for the service name                                                                                      |
| `--pod`            | `test.pod`                                                                | The prefix for the pod name                                                                                          |
| `--namespaces`     | 2                                                                         | The number of namespaces in cluster                                                                                  |
| `--services`       | 3                                                                         | The number of services for each namespace in cluster                                                                 |
| `--pods`           | 1                                                                         | The number of pods for each service in cluster                                                                       |
| `--calls`          | 100                                                                       | The number of calls generated by each pod in 5 minutes                                                               |
| `--pg.url`         | `postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_URL/$POSTGRES_DB` | Full connection string (with credentials) to Postgres instance                                                       |
| `--pg.ssl_mode`    | `prefer`                                                                  | SSL mode for PG connections. Possible values: `disable`, `allow`, `prefer`, `require`, `verify-ca`, `verify-full`    |
| `--pg.ca_file`     | ""                                                                        | Path to custom CA certificate for PG                                                                                 |
| `--minio.url`      | `$MINIO_ENDPOINT`                                                         | Url to Minio instance                                                                                                |
| `--minio.key`      | `$MINIO_ACCESS_KEY_ID`                                                    | Minio access key                                                                                                     |
| `--minio.secret`   | `$MINIO_SECRET_ACCESS_KEY`                                                | Minio secret key                                                                                                     |
| `--minio.bucket`   | `$MINIO_BUCKET`                                                           | Bucket in Minio                                                                                                      |
| `--minio.insecure` | `false`                                                                   | Use flag insecure for Minio                                                                                          |
| `--minio.use_ssl`  | `false`                                                                   | Use SSL access for Minio                                                                                             |
| `--minio.ca_file`  | ""                                                                        | Path to custom CA certificate for Minio                                                                              |

**NOTE:**

- `startdate`, `enddate`: dates in date in `yyyy/mm/dd` format;  
  **NOTE:** time range should not be more than `31` days
- `starttime`: hour of day in `yyyy/mm/dd/hh` format
- If one of S3-related parameters is missing, then uploading to S3 would be disabled
- If one of Postgres connection url is missing, then statuses of generated files in Postgres would not be updated

### Environment Variables

Instead of providing credentials as cmd line arguments, these environment variables could be defined for working with
S3-compatible storage and/or Postgres:

- `MINIO_ENDPOINT`
- `MINIO_ACCESS_KEY_ID`
- `MINIO_SECRET_ACCESS_KEY`
- `MINIO_BUCKET`
- `POSTGRES_URL`
- `POSTGRES_USER`
- `POSTGRES_PASSWORD`
- `POSTGRES_DB`

If `MINIO_BUCKET` is not specified, default value `profiler` will be used.

#### Examples for settings

| Command          | Default                    | ExampleDescription                              |
|------------------|----------------------------|-------------------------------------------------|
| `--pg.url`       | `$POSTGRES_URL`            | `postgresql://user:pass@localhost:5432/db_name` |
| `--minio.url`    | `$MINIO_ENDPOINT`          | `localhost:9000`                                |
| `--minio.key`    | `$MINIO_ACCESS_KEY_ID`     | `test`                                          |
| `--minio.secret` | `$MINIO_SECRET_ACCESS_KEY` | `secret_key`                                    |

Connection to Postgres DB in test cloud environments:

- Host - node ip where Postgres pod with label `pgtype=master` is running
- Port - `nodePort` specified in service
- Database should be already created
- Tables should be imported
  from [schema](https://github.com/Netcracker/qubership-profiler-backend/apps/data-generator/-/blob/master/.docker/postgres/opt/cdt_schema.sql).

---

## Use Cases

**Note**: In all following cases it is assumed that the parameters related to the PG
or minio are defined in the environment variables.
That's why they are omitted from the cli arguments in examples.
See [Environment Variables](#environment-variables) for details.

### Minimal run for Minio

Minimal run requires only `startdate` and `enddate` cli options specified.
In that case date will be uploaded only to s3 storage:

```bash
./data-generator run --startdate=2024/06/01 --enddate=2024/06/02
```

For that run:

1. 2 namespaces, 3 services in each namespace and 1 replica pod for every service;
2. 100 calls per 5 minutes;
3. Data for s3 storage is generated from 01 June 2024 to 02 June 2024;
4. Recent data for PG is not generated;

### Minimal run PG + Minio

To generate recent data for PG `starttime` should be additionally specified. Also, you can specify the number
of hours for PG generation using `hours` option:

```bash
./data-generator run --startdate=2024/06/01 --enddate=2024/06/02 --starttime=2024/06/10/07 --hours=3
```

For that run:

1. 2 namespaces, 3 services in each namespace and 1 replica pod for every service;
2. 100 calls per 5 minutes;
3. Data for s3 storage is generated from 01 June 2024 to 02 June 2024;
4Recent data for PG is generated from 10 June 2024 07:00 UTC for next 3 hours;

### Big generation

You can manipulate with the number of namespaces/services/pods using `namespaces`/`services`/`pods` options.
It can be useful to emulate big load on minio/PG.
**Note**: can take some time for generation:

```bash
./data-generator run --startdate=2024/05/01 --enddate=2024/05/30 --starttime=2024/06/10/07 --hours 4 --namespaces=40 --services=10 --pods=5
```

For that run:

1. 40 namespaces, 10 services in each namespace and 5 replica pod for every service;
2. 100 calls per 5 minutes;
3. Data for s3 storage is generated from 01 May 2024 to 30 May 2024;
4Recent data for PG is generated from 10 June 2024 07:00 UTC for next 4 hours;

## How to read Parquet files

### [UI] bigdata-file-viewer

Java GUI application

[Download](https://github.com/Eugene-Mark/bigdata-file-viewer) a jar file, run with

```bash
javaw -jar BigdataFileViewer-1.3-SNAPSHOT-jar-with-dependencies.jar
```

open the file you are interested.

![bigdata_viewer_screen.png](doc/images/bigdata_file_viewer.png)

### Clickhouse

- [Download](https://clickhouse.com/docs/en/operations/utilities/clickhouse-local)
  and [install](https://clickhouse.com/docs/en/integrations/sql-clients/clickhouse-client-local) local Clickhouse as CLI
  application.
- figure out which columns are in the Parquet file:

  ```bash
  clickhouse-local -q "DESCRIBE TABLE file('test.parquet', Parquet)"
  ```

- get count of rows in the Parquet file:

  ```bash
  clickhouse-local -q "SELECT COUNT() FROM 'test.parquet'"
  ```

- list first 20 rows from the Parquet file:

  ```bash
  clickhouse-local -q "SELECT time, namespace, service, pod, restartTime, method FROM file('test.parquet') LIMIT 20"
  ```

- read all the CDT's Parquet files that match the pattern:

  ```bash
  clickhouse-local -q "SELECT DISTINCT namespace, service, pod FROM file('test_namespace-*.parquet')
  ORDER BY namespace, service, pod"
  ```

- read from S3 over https :

  ```bash
  clickhouse-local -q "SELECT count() FROM s3('https://s3_url/some_file.parquet')"
  ```

- read from Minio (with credentials):

  ```bash
  clickhouse-local -q "SELECT count()
  FROM s3('http://127.0.0.1:9000/profiler/2024/02/*/*/test_namespace-*_calls.origin.parquet', 'test', 'password')"
  ```

Related to CDT's Parquet files:

- list unique services and pod names from the CDT's Parquet file:

  ```bash
  clickhouse-local -q "SELECT DISTINCT namespace, service, pod FROM file('test_namespace-1.parquet')"
  ```

- filter and reorder rows from the Parquet file:

  ```bash
  clickhouse-local -q "SELECT time, namespace, service, pod, restartTime FROM file('test_namespace-1.parquet')
  WHERE service = 'test_service-2' ORDER BY time ASC LIMIT 10"
  ```

- make sure durations are correct in shared calls file for specified duration range:

  ```bash
  SELECT min(duration), max(duration) FROM file('test_namespace-1-10ms.parquet')
  ```

See also [documentation](https://clickhouse.com/docs/en/integrations/data-formats/parquet)
and [examples](https://clickhouse.com/blog/apache-parquet-clickhouse-local-querying-writing) from official blog.

### DuckDB

[Download](https://duckdb.org/docs/installation/index?version=latest&environment=cli&installer=binary&platform=win),
install and run DuckDB standalone CLI application.

- figure out which columns/types are in the Parquet file:

  ```bash
  DESCRIBE SELECT * FROM 'test.parquet';
  ```

- get count of rows in the Parquet file:

  ```bash
  SELECT COUNT(*) FROM 'test.parquet';
  ```

- list first 20 rows from the Parquet file:

  ```bash
  SELECT * FROM 'test.parquet' LIMIT 20;
  ```

- read all files that match the pattern:

  ```bash
  SELECT * FROM 'test/*.parquet';
  ```

- read over https

  ```bash
  SELECT * FROM read_parquet('https://some.url/some_file.parquet');
  ```

**NOTE:** Don't forget to put a semicolon on the end of the command!

See also [documentation](https://duckdb.org/docs/data/parquet/overview.html)
